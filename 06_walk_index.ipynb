{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926e01d2-c664-492e-ac95-93db145fa41d",
   "metadata": {},
   "source": [
    "This notebook contain code on calculating and analysing walkability index for various unit of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c835fc7-4242-42f6-a7e1-dd81fa7aa7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import seaborn as sns\n",
    "from geocube.api.core import make_geocube\n",
    "import rasterio\n",
    "import glob\n",
    "import pickle\n",
    "from shapely.ops import unary_union\n",
    "from shapely import Polygon\n",
    "from statistics import mode\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from libpysal.weights import Queen\n",
    "from esda.moran import Moran\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a83761-134d-4e67-922c-1e3187eb6057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for grid files\n",
    "# you might have grids generated here from the 'generate_grids.ipynb'\n",
    "grid_path = 'data/*.parquet'\n",
    "\n",
    "# create a list of all parquet grid files from the specified directory\n",
    "grids_list = [parquet for parquet in glob.glob(grid_path)]\n",
    "print(grids_list)\n",
    "\n",
    "# # configure logging (recommended if you monitor processing over a lot of files)\n",
    "# log_path = 'logs/walk_index.log'\n",
    "\n",
    "# # ensure log directory exists\n",
    "# log_dir = os.path.dirname(log_path)\n",
    "# if not os.path.exists(log_dir):\n",
    "#     os.makedirs(log_dir)\n",
    "    \n",
    "# logging.basicConfig(filename=log_path, level=logging.INFO,\n",
    "#                     format='%(asctime)s:%(levelname)s:%(message)s', force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6980e25-dc40-4ba3-a226-bb8272202685",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nuts 3 analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e1daba-dde5-4d48-8619-ed83ea2123dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_gdf = gpd.read_file('data/NUTS_RG_01M_2024_3035.geojson')\n",
    "nuts3_gdf = nuts_gdf[nuts_gdf.LEVL_CODE == 3]\n",
    "\n",
    "# clip by osm eu bbox\n",
    "osm_eu_gdf = gpd.read_file('data/osm_eu_bbox.shp').to_crs('epsg:3035')\n",
    "eu_gdf = nuts3_gdf[nuts3_gdf.intersects(osm_eu_gdf.geometry.values[0])]\n",
    "\n",
    "# open 100km grid file\n",
    "grids_gdf = gpd.read_file('data/grid_100km_surf.gpkg')\n",
    "grids_gdf.index += 1\n",
    "\n",
    "# join for grid_ids and corresponding country polygon\n",
    "eu_gdf_with_grids = gpd.sjoin(eu_gdf, grids_gdf[['geometry']], how='left', predicate='intersects')\n",
    "eu_gdf_with_grids.rename(columns ={'index_right': 'grid_id'}, inplace=True)\n",
    "\n",
    "# filter for a list of countries\n",
    "# test_gdf = eu_gdf_with_grids.loc[eu_gdf_with_grids.CNTR_CODE.isin(['NL'])]\n",
    "# grouped_test_gdf = eu_gdf_with_grids.groupby('CNTR_CODE')\n",
    "eu_gdf_with_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48257aa-ec9b-4ff9-99a7-3b0907537740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive function? not sure if this speeds up!\n",
    "def process_window(geometry_arr, index_arr, x, y, window_size, poly_gdf, walk_gdf_list, merged_gdf):\n",
    "    if window_size < 1:  # minimal window size to avoid too deep recursion\n",
    "        return\n",
    "\n",
    "    # extract the window\n",
    "    window = geometry_arr[x:x+window_size, y:y+window_size]\n",
    "    window_index = index_arr[x:x+window_size, y:y+window_size]\n",
    "    \n",
    "    # flatten the array of polygons to a list\n",
    "    polygons = [poly for row in window for poly in row if poly is not None]\n",
    "    if polygons:\n",
    "        combined_polygon = unary_union(polygons)\n",
    "        \n",
    "        # intersection and containment checks\n",
    "        if combined_polygon.intersects(poly_gdf.geometry.values[0]):\n",
    "            if combined_polygon.within(poly_gdf.geometry.values[0]):\n",
    "                # add to the list if within the specified geometry\n",
    "                walk_gdf_list.append(merged_gdf.loc[list(window_index.flatten())])\n",
    "            else:\n",
    "                # recurse with smaller window sizes\n",
    "                if window_size == 1:\n",
    "                    if window[0][0].intersects(poly_gdf.geometry.values[0]):\n",
    "                        walk_gdf_list.append(merged_gdf.loc[list(window_index.flatten())])\n",
    "                else:\n",
    "                    new_window_size = window_size // 5 if window_size > 4 else window_size // 4\n",
    "                    for new_x in range(x, x+window_size, new_window_size):\n",
    "                        for new_y in range(y, y+window_size, new_window_size):\n",
    "                            process_window(geometry_arr, index_arr, new_x, new_y, new_window_size, poly_gdf, walk_gdf_list, merged_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef008395-d816-492f-9c7d-ffbdbf7f8c1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_gdf = eu_gdf_with_grids.groupby('NUTS_ID')\n",
    "\n",
    "for group_id, gdf in grouped_gdf:\n",
    "    walk_gdf_list = []\n",
    "    if os.path.exists(f'data/walkability/grids_nuts3/grids_{str(group_id)}.parquet'):\n",
    "        print(f'nuts 3 grids already calculated, skipping {group_id}')\n",
    "    else: \n",
    "        for i, row in gdf.iterrows():\n",
    "            print(row.NUTS_ID, row.grid_id)\n",
    "            if pd.isna(row.grid_id):\n",
    "                print(f'NAN grid_id, skipping...')\n",
    "                continue\n",
    "            try:\n",
    "                df = pd.read_csv(f'data/processed_data/processed_data/processed_data_{int(row.grid_id)}.csv')\n",
    "            except Exception as e:\n",
    "                print(f'error {e} at {int(row.grid_id)}, skipping...')\n",
    "                continue\n",
    "        \n",
    "            df_f = df[:1_000_000]    # gpu threads > 1_000_000 are irrelevant!\n",
    "            df_ff = df_f.copy()\n",
    "            df_ff.loc[df_ff['ndvi'] < 0, 'ndvi'] = 0\n",
    "            df_ff.loc[df_ff['population'] < 0, 'population'] = 0\n",
    "            df_ff['ent_5'] = df_ff['ent_5'].apply(lambda x: 0 if x < 0 or pd.isna(x) else x)\n",
    "            df_ff['slope'] = df_ff['slope'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "        \n",
    "            # open the corresponding iso_gdf for iso area weighing\n",
    "            iso_gdf = gpd.read_parquet(f'data/isochrones/isochrones_{int(row.grid_id)}.parquet')\n",
    "            iso_gdf['iso_area'] = iso_gdf.area/1e6\n",
    "            iso_gdf_f = iso_gdf[['index', 'iso_area']].copy()\n",
    "        \n",
    "            # open the corresponding grid gdf for geometries\n",
    "            grid_gdf = gpd.read_parquet(f'data/grids_100_{int(row.grid_id)}.parquet')\n",
    "            grid_gdf_f = grid_gdf[['index', 'grid_100000_id', 'geometry', 'population']].copy()\n",
    "            grid_gdf_f = grid_gdf_f.rename(columns={'population': 'population_full'})\n",
    "            grid_gdf_f.loc[grid_gdf_f['population_full'] < 0, 'population_full'] = 0\n",
    "        \n",
    "            # merge df, gdf, and iso_gdf\n",
    "            temp_merged_gdf = grid_gdf_f.merge(iso_gdf_f, on='index')\n",
    "            merged_gdf = temp_merged_gdf.merge(df_ff, on='index')\n",
    "        \n",
    "            bounds = merged_gdf.total_bounds\n",
    "            temp_polygon = Polygon([\n",
    "                (bounds[0], bounds[1]),  # (minx, miny)\n",
    "                (bounds[0], bounds[3]),  # (minx, maxy)\n",
    "                (bounds[2], bounds[3]),  # (maxx, maxy)\n",
    "                (bounds[2], bounds[1]),  # (maxx, miny)\n",
    "                (bounds[0], bounds[1])   # closing the polygon at the starting point\n",
    "            ])\n",
    "        \n",
    "            if temp_polygon.within(gdf.geometry.values[0]) == True:\n",
    "                walk_gdf_list.append(merged_gdf)\n",
    "            else:\n",
    "                merged_gdf_f = merged_gdf[merged_gdf.intersects(gdf.geometry.values[0])]\n",
    "                walk_gdf_list.append(merged_gdf_f)\n",
    "                \n",
    "                # recursion?\n",
    "                # index_arr = np.flipud(merged_gdf['index'].to_numpy().reshape(1000, 1000).T)\n",
    "                # geometry_arr = np.flipud(merged_gdf['geometry'].to_numpy().reshape(1000, 1000).T)\n",
    "                # window_size = 500\n",
    "                # width, height = geometry_arr.shape\n",
    "\n",
    "                # for x in range(0, width, window_size):\n",
    "                #     for y in range(0, height, window_size):\n",
    "                #         process_window(geometry_arr, index_arr, x, y, window_size, gdf, walk_gdf_list, merged_gdf)\n",
    "                        \n",
    "        try:\n",
    "            walk_gdf = gpd.GeoDataFrame(pd.concat(walk_gdf_list, ignore_index=True))\n",
    "            walk_gdf.to_parquet(f'data/walkability/grids_nuts3/grids_{str(group_id)}.parquet')\n",
    "            print(walk_gdf)\n",
    "        except Exception as e:\n",
    "            print(f'skipping {group_id} because of {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bba1ef3-99cb-4095-b1de-d7ee380a1de9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the index\n",
    "p_cols = ['street_walk_length', 'iso_area', 'num_street_intersections', 'pub_trans_count', 'ndvi', 'slope', 'ent_5']\n",
    "data_dict = {}\n",
    "\n",
    "grouped_gdf = eu_gdf_with_grids.groupby('NUTS_ID')\n",
    "\n",
    "for group_id, gdf in grouped_gdf:\n",
    "    print(str(group_id))\n",
    "    try:\n",
    "        grid_gdf = gpd.read_parquet(f'data/walkability/grids_nuts3/grids_{str(group_id)}.parquet')\n",
    "    except Exception as e:\n",
    "        print(f'skipping {str(group_id)} due to error {e}')\n",
    "        continue\n",
    "\n",
    "    # nuts_3_area = grid_gdf.area.sum()/1e6\n",
    "    # print(len(grid_gdf)/1e2)\n",
    "    for col in p_cols:\n",
    "        pop_weighted_value = (grid_gdf['population_full']*grid_gdf[col]).sum() / grid_gdf['population_full'].sum()\n",
    "        if str(group_id) not in data_dict:\n",
    "            data_dict[str(group_id)] = {f'{col}_pop_w': pop_weighted_value}\n",
    "        else:\n",
    "            data_dict[str(group_id)][f'{col}_pop_w'] = pop_weighted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b55652-4e50-480c-b4f9-7ff668300c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb1e5c-1164-4c15-9faf-cdfc7bf5dc83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict_f = {key: value for key, value in data_dict.items() if not key.startswith('TR')}\n",
    "data_dict_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7cb0b-76d7-4732-949a-a6ef3b52d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict_f).T\n",
    "\n",
    "# Apply z-score standardization\n",
    "standardized_df = df.apply(zscore)\n",
    "standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b17561-95c0-48a8-ac77-b668f732e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_df['walkability'] = (standardized_df['street_walk_length_pop_w'] +\n",
    "                                  standardized_df['iso_area_pop_w'] +\n",
    "                                 standardized_df['num_street_intersections_pop_w'] +\n",
    "                                 standardized_df['ndvi_pop_w'] +\n",
    "                                 standardized_df['ent_5_pop_w'] +\n",
    "                                 (-standardized_df['slope_pop_w']) +\n",
    "                                 standardized_df['pub_trans_count_pop_w'])\n",
    "standardized_df['walk_index'] = ((standardized_df['walkability'] - standardized_df['walkability'].min()) / (standardized_df['walkability'].max() - standardized_df['walkability'].min()))*100\n",
    "standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240477e-8428-4a05-8193-f8d186fbb639",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_labels = [i+1 for i in range(10)]\n",
    "standardized_df['walk_decile'] = pd.qcut(standardized_df['walkability'], 10, labels=decile_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31439fb-43c1-4826-b7ef-25e3a8596140",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_df.reset_index(inplace=True)\n",
    "standardized_df.rename(columns={'index': 'NUTS_ID'}, inplace=True)\n",
    "\n",
    "standardized_gdf = nuts3_gdf.merge(standardized_df, on='NUTS_ID', how='inner')\n",
    "standardized_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa41ba-fbef-4b4f-8000-1a849e4b86b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_gdf[standardized_gdf.walk_index >= 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a0e393-3e34-4c64-b7e2-22d4d4b2cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_gdf.to_parquet('data/walkability/nuts3_walk.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516bfe7f-a485-4d8d-86e9-94d08020bcf8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### by lau analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee06fb-342e-4db7-857e-0fb9ca807d1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lau_gdf = gpd.read_file('data/LAU_RG_01M_2023_3035.geojson')\n",
    "lau_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0383a033-1fc9-4e29-b374-e511d9992cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip by osm eu bbox\n",
    "osm_eu_gdf = gpd.read_file('data/osm_europe/osm_eu_bbox.shp').to_crs('epsg:3035')\n",
    "eu_gdf = lau_gdf[lau_gdf.intersects(osm_eu_gdf.geometry.values[0])]\n",
    "\n",
    "# open 100km grid file\n",
    "grids_gdf = gpd.read_file('data/grid_100km_surf.gpkg')\n",
    "grids_gdf.index += 1\n",
    "\n",
    "# join for grid_ids and corresponding country polygon\n",
    "eu_gdf_with_grids = gpd.sjoin(eu_gdf, grids_gdf[['geometry']], how='left', predicate='intersects')\n",
    "eu_gdf_with_grids.rename(columns ={'index_right': 'grid_id'}, inplace=True)\n",
    "\n",
    "# filter for a list of countries\n",
    "# test_gdf = eu_gdf_with_grids.loc[eu_gdf_with_grids.CNTR_CODE.isin(['NL'])]\n",
    "# grouped_test_gdf = eu_gdf_with_grids.groupby('CNTR_CODE')\n",
    "eu_gdf_with_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da1b23-7802-4678-ab97-c7bc90b45b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_gdf = eu_gdf_with_grids.groupby('GISCO_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46359a7-a0ef-4c4e-aadb-539de06d1057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize cache variables\n",
    "cached_grid_id = None\n",
    "cached_df = None\n",
    "cached_iso_gdf = None\n",
    "cached_grid_gdf = None\n",
    "\n",
    "for group_id, gdf in grouped_gdf:\n",
    "    walk_gdf_list = []\n",
    "\n",
    "    if os.path.exists(f'data/walkability/grids_lau/grids_{str(group_id)}.parquet'):\n",
    "        print(f'lau grids already calculated, skipping {group_id}')\n",
    "    else: \n",
    "        for i, row in gdf.iterrows():\n",
    "            print(row.GISCO_ID, row.grid_id)\n",
    "            if pd.isna(row.grid_id):\n",
    "                print(f'NAN grid_id, skipping...')\n",
    "                continue\n",
    "\n",
    "            # cache check and loading data\n",
    "            if row.grid_id != cached_grid_id:\n",
    "                try:\n",
    "                    df = pd.read_csv(f'data/processed_data/processed_data/processed_data_{int(row.grid_id)}.csv')\n",
    "                    iso_gdf = gpd.read_parquet(f'data/isochrones/isochrones_{int(row.grid_id)}.parquet')\n",
    "                    grid_gdf = gpd.read_parquet(f'data/grids_100_{int(row.grid_id)}.parquet')\n",
    "\n",
    "                    cached_grid_id = row.grid_id\n",
    "                    cached_df = df\n",
    "                    cached_iso_gdf = iso_gdf\n",
    "                    cached_grid_gdf = grid_gdf\n",
    "                except Exception as e:\n",
    "                    print(f'error {e} at {int(row.grid_id)}, skipping...')\n",
    "                    continue\n",
    "            else:\n",
    "                print(f'using cached data for {row.GISCO_ID, row.grid_id}')\n",
    "                df = cached_df\n",
    "                iso_gdf = cached_iso_gdf\n",
    "                grid_gdf = cached_grid_gdf\n",
    "\n",
    "            # process the data\n",
    "            df_f = df[:1_000_000]  # gpu threads > 1_000_000 are irrelevant!\n",
    "            df_ff = df_f.copy()\n",
    "            df_ff.loc[df_ff['ndvi'] < 0, 'ndvi'] = 0\n",
    "            df_ff.loc[df_ff['population'] < 0, 'population'] = 0\n",
    "            df_ff['ent_5'] = df_ff['ent_5'].apply(lambda x: 0 if x < 0 or pd.isna(x) else x)\n",
    "            df_ff['slope'] = df_ff['slope'].apply(lambda x: 0 if pd.isna(x) else x)\n",
    "        \n",
    "            iso_gdf['iso_area'] = iso_gdf.area / 1e6\n",
    "            iso_gdf_f = iso_gdf[['index', 'iso_area']].copy()\n",
    "        \n",
    "            grid_gdf_f = grid_gdf[['index', 'grid_100000_id', 'geometry', 'population']].copy()\n",
    "            grid_gdf_f = grid_gdf_f.rename(columns={'population': 'population_full'})\n",
    "            grid_gdf_f.loc[grid_gdf_f['population_full'] < 0, 'population_full'] = 0\n",
    "\n",
    "            temp_merged_gdf = grid_gdf_f.merge(iso_gdf_f, on='index')\n",
    "            merged_gdf = temp_merged_gdf.merge(df_ff, on='index')\n",
    "\n",
    "            # filter by intersection with the geometry of the current group\n",
    "            merged_gdf_f = merged_gdf[merged_gdf.intersects(gdf.geometry.values[0])]\n",
    "            walk_gdf_list.append(merged_gdf_f)\n",
    "                    \n",
    "        try:\n",
    "            walk_gdf = gpd.GeoDataFrame(pd.concat(walk_gdf_list, ignore_index=True))\n",
    "            walk_gdf.to_parquet(f'data/walkability/grids_lau/grids_{str(group_id)}.parquet')\n",
    "        except Exception as e:\n",
    "            print(f'skipping {group_id} because of {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ce195-14f5-456f-851c-1050eee635f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build the index\n",
    "p_cols = ['street_walk_length', 'iso_area', 'num_street_intersections', 'pub_trans_count', 'ndvi', 'slope', 'ent_5']\n",
    "data_dict = {}\n",
    "\n",
    "for group_id, gdf in grouped_gdf:\n",
    "    print(str(group_id))\n",
    "    try:\n",
    "        grid_gdf = gpd.read_parquet(f'data/walkability/grids_lau/grids_{str(group_id)}.parquet')\n",
    "    except Exception as e:\n",
    "        print(f'skipping {str(group_id)} due to error {e}')\n",
    "        continue\n",
    "\n",
    "    # add degree of urbanization\n",
    "    dou_data = []\n",
    "    if len(set(grid_gdf.grid_100000_id.values)) != 1:\n",
    "        grouped_grid_gdf = grid_gdf.groupby('grid_100000_id')\n",
    "        for grid_id, sub_grouped_grid_gdf in grouped_grid_gdf:\n",
    "            indices_oi = list(sub_grouped_grid_gdf['index'].values)\n",
    "            \n",
    "            grid_100k_gdf = gpd.read_parquet(f'data/grids_100_{grid_id}.parquet')  \n",
    "            dou_data.append(list(grid_100k_gdf.loc[indices_oi, 'dou'].values))\n",
    "\n",
    "    else:\n",
    "        indices_oi = list(grid_gdf['index'].values)\n",
    "        \n",
    "        grid_id = grid_gdf.grid_100000_id.values[0]\n",
    "        grid_100k_gdf = gpd.read_parquet(f'data/grids_100_{grid_id}.parquet')  \n",
    "        dou_data.append(list(grid_100k_gdf.loc[indices_oi, 'dou'].values))\n",
    "        \n",
    "    mode_dou = mode([element for sublist in dou_data for element in (sublist if isinstance(sublist, list) else [sublist])])\n",
    "    print(mode_dou)\n",
    "    data_dict[str(group_id)] = {'dou': mode_dou}\n",
    "    \n",
    "    # nuts_3_area = grid_gdf.area.sum()/1e6\n",
    "    # print(len(grid_gdf)/1e2)\n",
    "    for col in p_cols:\n",
    "        pop_weighted_value = (grid_gdf['population_full']*grid_gdf[col]).sum() / grid_gdf['population_full'].sum()\n",
    "        # if str(group_id) not in data_dict:\n",
    "        #     data_dict[str(group_id)] = {f'{col}_pop_w': pop_weighted_value}\n",
    "        # else:\n",
    "        data_dict[str(group_id)][f'{col}_pop_w'] = pop_weighted_value\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d28213-6992-4654-972f-8be8832c69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save results to a pickle file so you don't have to process them again\n",
    "# pickle.dump(data_dict, open('/Volumes/ssd1/eurostat_grid/grids_100/logs/data_grids_lau.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9af11ae-723e-44ac-acdf-fea935a3b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logs/data_grids_lau.p', 'rb') as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ed708-33e4-428e-a9f8-6a96bfa78915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e5849-fc5a-4ec0-a72c-27cbaa0277af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict).T\n",
    "df.dropna(inplace=True)\n",
    "df = df[df.dou != 10]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f14f96-bdec-49dd-8ff8-ca499bf20971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply z-score standardization\n",
    "columns_to_zscore = ['street_walk_length_pop_w', 'iso_area_pop_w',\n",
    "                     'num_street_intersections_pop_w', 'pub_trans_count_pop_w',\n",
    "                     'ndvi_pop_w', 'slope_pop_w', 'ent_5_pop_w']\n",
    "\n",
    "df[columns_to_zscore] = df[columns_to_zscore].apply(zscore)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead33a37-f6eb-4e22-842f-301f4393a616",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['walkability'] = (df['street_walk_length_pop_w'] +\n",
    "                     df['iso_area_pop_w'] +\n",
    "                     df['num_street_intersections_pop_w'] +\n",
    "                     df['ndvi_pop_w'] +\n",
    "                     df['ent_5_pop_w'] +\n",
    "                     (-df['slope_pop_w']) +\n",
    "                     df['pub_trans_count_pop_w'])\n",
    "df['walk_index'] = ((df['walkability'] - df['walkability'].min()) / (df['walkability'].max() - df['walkability'].min()))*100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35533c5-0ce5-4142-973c-5a44c0dfbefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_labels = [i+1 for i in range(10)]\n",
    "df['walk_decile'] = pd.qcut(df['walkability'], 10, labels=decile_labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be5ebe-f509-48b4-8d05-7a6e955600a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.rename(columns={'index': 'GISCO_ID'}, inplace=True)\n",
    "\n",
    "gdf = lau_gdf.merge(df, on='GISCO_ID', how='inner')\n",
    "gdf.dropna(inplace=True)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f005fb-9fab-4faf-9470-fe53adcbfd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "\n",
    "# gdf.to_parquet('data/walkability/lau_walk.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef0662-39a5-4285-bea8-955baba6f93f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bd4a82-46a8-488d-8b8d-1041b969f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# load the data (replace with actual file path)\n",
    "gdf = gpd.read_parquet('data/walkability/lau_walk.parquet')\n",
    "\n",
    "# define numeric columns and their abbreviations\n",
    "numeric_cols = ['street_walk_length_pop_w', 'iso_area_pop_w',\n",
    "                'num_street_intersections_pop_w', 'pub_trans_count_pop_w',\n",
    "                'ndvi_pop_w', 'slope_pop_w', 'ent_5_pop_w', 'walkability']\n",
    "\n",
    "abbreviations = {\n",
    "    'street_walk_length_pop_w': 'SWL',\n",
    "    'iso_area_pop_w': 'ISO',\n",
    "    'num_street_intersections_pop_w': 'SI',\n",
    "    'pub_trans_count_pop_w': 'PT',\n",
    "    'ndvi_pop_w': 'GS',\n",
    "    'slope_pop_w': 'SLOPE',\n",
    "    'ent_5_pop_w': 'LUM',\n",
    "    'walkability': 'WALK'\n",
    "}\n",
    "\n",
    "# eename columns\n",
    "gdf.rename(columns=abbreviations, inplace=True)\n",
    "\n",
    "# compute correlation matrix\n",
    "corr = gdf[list(abbreviations.values())].corr(method='spearman')\n",
    "\n",
    "# mask and melt correlation matrix\n",
    "mask = np.tril(np.ones_like(corr, dtype=bool))\n",
    "melt = corr.mask(mask).melt(ignore_index=False).reset_index()\n",
    "melt[\"size\"] = melt[\"value\"].abs()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# normalize colorbar\n",
    "cmap = plt.cm.RdBu\n",
    "norm = plt.Normalize(-1, 1)\n",
    "sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.ax.tick_params(labelsize=\"x-small\")\n",
    "\n",
    "# plot lower triangle (scatter plot with normalized hue and square markers)\n",
    "sns.scatterplot(ax=ax, data=melt, x=\"index\", y=\"variable\", size=\"size\",\n",
    "                hue=\"value\", hue_norm=norm, palette=cmap,\n",
    "                style=0, markers=[\"s\"], legend=False)\n",
    "\n",
    "# format grid\n",
    "xmin, xmax = (-0.5, corr.shape[0] - 0.5)\n",
    "ymin, ymax = (-0.5, corr.shape[1] - 0.5)\n",
    "ax.vlines(np.arange(xmin, xmax + 1), ymin, ymax, lw=1, color=\"silver\")\n",
    "ax.hlines(np.arange(ymin, ymax + 1), xmin, xmax, lw=1, color=\"silver\")\n",
    "ax.set(aspect=1, xlim=(xmin, xmax), ylim=(ymax, ymin), xlabel=\"\", ylabel=\"\")\n",
    "ax.tick_params(labelbottom=False, labeltop=True)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# annotate upper triangle\n",
    "for y in range(corr.shape[0]):\n",
    "    for x in range(corr.shape[1]):\n",
    "        value = corr.mask(mask).to_numpy()[y, x]\n",
    "        if pd.notna(value):\n",
    "            plt.text(x, y, f\"{value:.2f}\", size=\"x-small\",\n",
    "                     ha=\"center\", va=\"center\")\n",
    "\n",
    "# plt.savefig('plots_paper/corrmatrix.png', dpi=720, bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e116b05-1a0b-4b9d-bdb6-64187f915ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed5e33-35b9-4c6c-a650-f36a60cce115",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791d9f1-23ea-46fa-9e24-0b5210ad00b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for pair plot\n",
    "\n",
    "sampled_gdf = gdf[gdf['dou'].isin([13, 21, 22, 23, 30])].copy()\n",
    "sampled_gdf['dou'] = sampled_gdf['dou'].astype('category')\n",
    "sampled_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4930e-7093-4356-aa32-7bd11b05a6ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create directory for plots\n",
    "output_dir = \"joint_plots\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# define variable pairs\n",
    "variable_pairs = [\n",
    "    ('SWL', 'WALK'),\n",
    "    ('ISO', 'WALK'),\n",
    "    ('SI', 'WALK'),\n",
    "    ('PT', 'WALK'),\n",
    "    ('GS', 'WALK'),\n",
    "    ('SLOPE', 'WALK'),\n",
    "    ('LUM', 'WALK'),\n",
    "]\n",
    "\n",
    "abbreviations = {\n",
    "    'street_walk_length_pop_w': 'SWL',\n",
    "    'iso_area_pop_w': 'ISO',\n",
    "    'num_street_intersections_pop_w': 'SI',\n",
    "    'pub_trans_count_pop_w': 'PT',\n",
    "    'ndvi_pop_w': 'GS',\n",
    "    'slope_pop_w': 'SLOPE',\n",
    "    'ent_5_pop_w': 'LUM',\n",
    "    'walkability': 'WALK'\n",
    "}\n",
    "\n",
    "# rename columns\n",
    "sampled_gdf.rename(columns=abbreviations, inplace=True)\n",
    "\n",
    "# define palette\n",
    "unique_categories = sampled_gdf['dou'].cat.categories if sampled_gdf['dou'].dtype.name == 'category' else sampled_gdf['dou'].unique()\n",
    "cmap = plt.get_cmap('Accent')\n",
    "palette = {category: cmap(i / len(unique_categories)) for i, category in enumerate(unique_categories)}\n",
    "\n",
    "# define legend labels\n",
    "dou_labels = {\n",
    "    13: \"Rural cluster\",\n",
    "    21: \"Suburban or peri-urban\",\n",
    "    22: \"Semi-dense urban cluster\",\n",
    "    23: \"Dense urban cluster\",\n",
    "    30: \"Urban centre\"\n",
    "}\n",
    "\n",
    "# generate and save plots\n",
    "for x_var, y_var in variable_pairs:\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    # sns.set_context(\"paper\", font_scale=1.5)\n",
    "    \n",
    "    # Create jointplot\n",
    "    joint = sns.jointplot(\n",
    "        data=sampled_gdf,\n",
    "        x=x_var,\n",
    "        y=y_var,\n",
    "        hue='dou',\n",
    "        palette=palette,\n",
    "        kind='scatter',\n",
    "        marker='+',\n",
    "        s=100,\n",
    "        alpha=0.5,\n",
    "        height=5,\n",
    "        ratio=5,\n",
    "        space=0.2,\n",
    "    )\n",
    "    \n",
    "    # add regression lines\n",
    "    for hue_val, color in palette.items():\n",
    "        subset = sampled_gdf[sampled_gdf['dou'] == hue_val]\n",
    "        sns.regplot(\n",
    "            data=subset,\n",
    "            x=x_var,\n",
    "            y=y_var,\n",
    "            scatter=False,\n",
    "            line_kws={'color': 'black', 'linewidth': 1, 'linestyle': '--', 'alpha': 0.8},\n",
    "            ax=joint.ax_joint\n",
    "        )\n",
    "        sns.regplot(\n",
    "            data=subset,\n",
    "            x=x_var,\n",
    "            y=y_var,\n",
    "            scatter=False,\n",
    "            line_kws={'color': color, 'linewidth': 2, 'alpha': 0.5},\n",
    "            ax=joint.ax_joint\n",
    "        )\n",
    "    \n",
    "    # set labels\n",
    "    joint.set_axis_labels(x_var, y_var, fontsize=20)\n",
    "    joint.ax_joint.tick_params(labelsize=20)\n",
    "    joint.ax_marg_x.tick_params(labelsize=20)\n",
    "    joint.ax_marg_y.tick_params(labelsize=20)\n",
    "    joint.ax_joint.get_legend().remove()\n",
    "    # # Rotate x-axis labels\n",
    "    # for label in joint.ax_joint.get_xticklabels():\n",
    "    #     label.set_rotation(45)\n",
    "    \n",
    "    # # create custom legend\n",
    "    # handles = [\n",
    "    #     plt.Line2D(\n",
    "    #         [0], [0],\n",
    "    #         marker='+',\n",
    "    #         color=cmap(i / len(unique_categories)),\n",
    "    #         linestyle='',\n",
    "    #         markersize=8\n",
    "    #     )\n",
    "    #     for i, _ in enumerate(unique_categories)\n",
    "    # ]\n",
    "    # labels = [dou_labels.get(cat, str(cat)) for cat in unique_categories]\n",
    "    \n",
    "    # # add legend to joint plot\n",
    "    # joint.ax_joint.legend(handles, labels, title='Dou', fontsize=8, title_fontsize=9, loc='upper right')\n",
    "    \n",
    "    # save as SVG\n",
    "    plot_filename = f\"{x_var}_vs_{y_var}.png\"\n",
    "    joint.savefig(os.path.join(output_dir, plot_filename), format=\"png\", bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba524b9-0bc8-4288-97a5-dc9672aa9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom legend for jointplot\n",
    "\n",
    "unique_categories = sampled_gdf['dou'].cat.categories if sampled_gdf['dou'].dtype.name == 'category' else sampled_gdf['dou'].unique()\n",
    "cmap = plt.get_cmap('Accent')\n",
    "\n",
    "# create custom legend handles\n",
    "handles = [\n",
    "    plt.Line2D(\n",
    "        [0], [0],\n",
    "        marker='+',\n",
    "        color=cmap(i / len(unique_categories)),\n",
    "        linestyle='',\n",
    "        markersize=30,  # Larger marker size for readability\n",
    "        markeredgewidth=4  # Bold marker edges\n",
    "    )\n",
    "    for i, _ in enumerate(unique_categories)\n",
    "]\n",
    "\n",
    "# create labels\n",
    "labels = [dou_labels.get(cat, str(cat)) for cat in unique_categories]\n",
    "\n",
    "# create figure for legend\n",
    "fig, ax = plt.subplots(figsize=(10, 1))  # Keep the width manageable, height minimal\n",
    "legend = ax.legend(\n",
    "    handles,\n",
    "    labels,\n",
    "    loc='center',\n",
    "    ncol=5,  # Arrange in a single row\n",
    "    fontsize=20,  # Larger font size for readability\n",
    "    handletextpad=1.0,  # Space between marker and text\n",
    "    columnspacing=2.0,\n",
    "    frameon=False# Space between columns\n",
    ")\n",
    "ax.axis('off')  # Turn off the axis\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the legend as an image\n",
    "plt.savefig(\"joint_plots/custom_legend.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b6051-f41c-4ae7-ac86-8f99058cccb3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### country analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d997d27-d51b-48aa-a51f-57f7316744bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# country-wise analysis\n",
    "nuts_gdf = gpd.read_file('data/NUTS_RG_01M_2024_3035.geojson')\n",
    "nuts3_gdf = nuts_gdf[nuts_gdf.LEVL_CODE == 3]\n",
    "\n",
    "# clip by osm eu bbox\n",
    "osm_eu_gdf = gpd.read_file('data/osm_europe/osm_eu_bbox.shp').to_crs('epsg:3035')\n",
    "eu_gdf = nuts3_gdf[nuts3_gdf.intersects(osm_eu_gdf.geometry.values[0])]\n",
    "\n",
    "eu_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c8a2b-5664-4394-b6cc-232f6472d579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_gdf = eu_gdf.groupby('CNTR_CODE')\n",
    "for group_id, _ in grouped_gdf:\n",
    "    print(group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff14bec-bd8f-4fde-8752-02cdeb42dcbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_gdf = eu_gdf.groupby('CNTR_CODE')\n",
    "\n",
    "for group_id, gdf in grouped_gdf:\n",
    "    # print(group_id, gdf)\n",
    "    if os.path.exists(f'data/walkability/grids_country/grids_{str(group_id)}.parquet'):\n",
    "        print(f'country grids already calculated, skipping {group_id}')\n",
    "    else:\n",
    "        country_nuts3_list = []\n",
    "        for i, row in gdf.iterrows():\n",
    "            try:\n",
    "                grids_nuts3_gdf = gpd.read_parquet(f'data/walkability/grids_nuts3/grids_{row.NUTS_ID}.parquet')\n",
    "            except Exception as e:\n",
    "                print(f'skipping {row.NUTS_ID} because of {e}')\n",
    "                \n",
    "            if len(set(grids_nuts3_gdf.grid_100000_id.values)) != 1:\n",
    "                grouped_grids_nuts3_gdf = grids_nuts3_gdf.groupby('grid_100000_id')\n",
    "                for grid_id, nuts3_grid_gdf in grouped_grids_nuts3_gdf:\n",
    "                    indices_to_join_on = list(nuts3_grid_gdf['index'].values)\n",
    "                    \n",
    "                    grid_gdf = gpd.read_parquet(f'data/grids_100_{grid_id}.parquet')  \n",
    "                    column_data = grid_gdf.loc[indices_to_join_on, 'dou']\n",
    "                    \n",
    "                    nuts3_grid_gdf['dou'] = nuts3_grid_gdf['index'].map(column_data)\n",
    "                    country_nuts3_list.append(nuts3_grid_gdf)\n",
    "            else:\n",
    "                indices_to_join_on = list(grids_nuts3_gdf['index'].values)\n",
    "                \n",
    "                grid_id = grids_nuts3_gdf.grid_100000_id.values[0]\n",
    "                grid_gdf = gpd.read_parquet(f'data/grids_100_{grid_id}.parquet')  \n",
    "                column_data = grid_gdf.loc[indices_to_join_on, 'dou']\n",
    "                \n",
    "                grids_nuts3_gdf['dou'] = grids_nuts3_gdf['index'].map(column_data)\n",
    "                country_nuts3_list.append(grids_nuts3_gdf)\n",
    "    \n",
    "        country_gdf = gpd.GeoDataFrame(pd.concat(country_nuts3_list, ignore_index=True))\n",
    "        country_gdf.to_parquet(f'data/walkability/grids_country/grids_{str(group_id)}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d521e-de4f-455f-814c-6887770c8a85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate index for each degree of urbanization\n",
    "\n",
    "country_gdf_paths = [parquet for parquet in glob.glob(\"data/walkability/grids_country/*.parquet\")]\n",
    "\n",
    "data_dict = {}\n",
    "p_cols = ['street_walk_length', 'iso_area', 'num_street_intersections', 'pub_trans_count', 'ndvi', 'slope', 'ent_5']\n",
    "\n",
    "for country_gdf_path in country_gdf_paths:\n",
    "    country_code = country_gdf_path.split('_')[-1].split('.')[0]\n",
    "    print(country_code)\n",
    "    country_gdf = gpd.read_parquet(country_gdf_path)\n",
    "    dou_grouped_country_gdf = country_gdf.groupby('dou')\n",
    "    inner_data_dict = {}\n",
    "    for dou, gdf in dou_grouped_country_gdf:\n",
    "        for col in p_cols:\n",
    "            total_pop = gdf['population_full'].sum()\n",
    "            if total_pop != 0:\n",
    "                pop_weighted_value = (gdf['population_full']*gdf[col]).sum() / total_pop\n",
    "            else:\n",
    "                pop_weighted_value = 0\n",
    "\n",
    "            if dou not in inner_data_dict:\n",
    "                inner_data_dict[dou] = {f'{col}_pop_w': pop_weighted_value}\n",
    "            else:\n",
    "                inner_data_dict[dou][f'{col}_pop_w'] = pop_weighted_value\n",
    "                \n",
    "    if country_code not in data_dict:\n",
    "        data_dict[country_code] = inner_data_dict\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65e722-2d3d-40c8-a106-d3e5f73d4155",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9becc9bb-342b-46ec-b1d6-d50c4f3b75f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate index for each degree of urbanization for EU-35 and EU-27\n",
    "\n",
    "# define your paths and columns\n",
    "country_gdf_paths = glob.glob(\"data/walkability/grids_country/*.parquet\")\n",
    "p_cols = ['street_walk_length', 'iso_area', 'num_street_intersections', 'pub_trans_count', 'ndvi', 'slope', 'ent_5']\n",
    "\n",
    "# initialize accumulators:\n",
    "#   - denoms will track the total population for each group\n",
    "#   - numerators will track the sum of (population * col) for each column and group\n",
    "denoms = defaultdict(float)\n",
    "numerators = defaultdict(lambda: {col: 0.0 for col in p_cols})\n",
    "\n",
    "# process each file one by one\n",
    "for country_gdf_path in country_gdf_paths:\n",
    "    country_code = country_gdf_path.split('_')[-1].split('.')[0]\n",
    "    print(\"Processing country code:\", country_code)\n",
    "    \n",
    "    # read the file (only one at a time)\n",
    "    gdf = gpd.read_parquet(country_gdf_path)\n",
    "    \n",
    "    # group the file by 'dou'\n",
    "    for dou, group in gdf.groupby('dou'):\n",
    "        pop_sum = group['population_full'].sum()\n",
    "        denoms[dou] += pop_sum  # update the denominator\n",
    "        \n",
    "        for col in p_cols:\n",
    "            # update the numerator: population-weighted sum for this column\n",
    "            numerators[dou][col] += (group['population_full'] * group[col]).sum()\n",
    "\n",
    "# now, compute the weighted averages for each group and each column\n",
    "data_dict = {'EU':{}}\n",
    "for dou in denoms:\n",
    "    data_dict['EU'][dou] = {}\n",
    "    for col in p_cols:\n",
    "        if denoms[dou] != 0:\n",
    "            data_dict['EU'][dou][f'{col}_pop_w'] = numerators[dou][col] / denoms[dou]\n",
    "        else:\n",
    "            data_dict['EU'][dou][f'{col}_pop_w'] = 0\n",
    "\n",
    "print(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5276755-8527-44c6-98d8-7cecb2d37d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(data_dict, open('logs/data_grids_country_full.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041346e1-4a7f-4b60-b6c4-e58af52ac828",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/ssd1/eurostat_grid/grids_100/logs/data_grids_country.p', 'rb') as f:\n",
    "    data_dict_1 = pickle.load(f)\n",
    "\n",
    "with open('/Volumes/ssd1/eurostat_grid/grids_100/logs/data_grids_country_full.p', 'rb') as f:\n",
    "    data_dict_2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae60739c-0ebf-4850-9a48-301fb2740889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = {**data_dict_1, **data_dict_2}\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fceceba-f569-48d6-b405-27e1a1faa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd620c-5ca9-4244-a25f-777acb3377de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the nested dictionary into a DataFrame\n",
    "df = pd.json_normalize(\n",
    "    [\n",
    "        {\"CNTR_CODE\": key, \"dou\": dou, **vals}\n",
    "        for key, dou_dict in data_dict.items()\n",
    "        for dou, vals in dou_dict.items()\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = df.loc[df['dou'] != 10]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a06fd31-f766-4fd8-bbd2-7242e519bb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply z-score standardization\n",
    "p_cols = ['street_walk_length', 'iso_area', 'num_street_intersections', 'pub_trans_count', 'ndvi', 'slope', 'ent_5']\n",
    "standardized_df = df.copy()\n",
    "\n",
    "for col in p_cols:\n",
    "    standardized_df[f'{col}_pop_w'] = zscore(df[f'{col}_pop_w'])\n",
    "\n",
    "standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbe962-887d-4595-a398-c62db247bea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_df['walkability'] = (standardized_df['street_walk_length_pop_w'] +\n",
    "                                  standardized_df['iso_area_pop_w'] +\n",
    "                                 standardized_df['num_street_intersections_pop_w'] +\n",
    "                                 standardized_df['ndvi_pop_w'] +\n",
    "                                 standardized_df['ent_5_pop_w'] +\n",
    "                                 (-standardized_df['slope_pop_w']) +\n",
    "                                 standardized_df['pub_trans_count_pop_w'])\n",
    "standardized_df['walk_index'] = ((standardized_df['walkability'] - standardized_df['walkability'].min()) / (standardized_df['walkability'].max() - standardized_df['walkability'].min()))*100\n",
    "\n",
    "decile_labels = [i+1 for i in range(10)]\n",
    "standardized_df['walk_decile'] = pd.qcut(standardized_df['walkability'], 10, labels=decile_labels)\n",
    "standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bc4d69-51d1-44a6-81cb-3f06066e8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardized_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d88c5b-298c-4c94-bd5d-56d5aed068d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the updated scatter plot with 'x' markers and dotted lines\n",
    "# plt.figure(figsize=(12, 8))\n",
    "\n",
    "# # Get unique DOU values and assign discrete colors\n",
    "# unique_dou = df['dou'].unique()\n",
    "# discrete_colors = plt.cm.get_cmap('tab10_r', len(unique_dou))\n",
    "\n",
    "# # Draw lines connecting points with the same DOU\n",
    "# for i, dou_value in enumerate(unique_dou):\n",
    "#     dou_df = standardized_df[standardized_df['dou'] == dou_value]  # Adjust to the correct DataFrame\n",
    "#     plt.plot(\n",
    "#         dou_df['CNTR_CODE'],\n",
    "#         dou_df['decile_class'],  # Replace 'iso_area_pop_w' with 'decile_class' if needed\n",
    "#         marker='x',\n",
    "#         linestyle='--',\n",
    "#         linewidth=0.5,\n",
    "#         color=discrete_colors(i),  # Map color from colormap\n",
    "#         markersize=4,  # Increase marker size\n",
    "#         markeredgewidth=1,  # Make marker edges bold\n",
    "#         label=f\"DOU {dou_value}\"\n",
    "#     )\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.xlabel('CNTR_CODE')\n",
    "# plt.ylabel('Walkability Decile')  # Replace with 'Walkability Decile' if needed\n",
    "# # plt.title('Comparison of ISO_AREA_POP_W across Countries with DOU Connection Lines (Dotted)')\n",
    "# plt.legend(title=\"Degree of Urbanization\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# plt.grid(False)\n",
    "# plt.gca().spines['top'].set_visible(False)\n",
    "# plt.gca().spines['right'].set_visible(False)\n",
    "# plt.tight_layout()\n",
    "\n",
    "# plt.savefig(\"test_plot.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "# # Display plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f2062-5d10-473b-b15f-f70439977a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a strip-like visualization for each country with points inside the strips\n",
    "# fig = plt.figure(figsize=(16, 8))\n",
    "\n",
    "# # Get unique countries and DOU values\n",
    "# unique_countries = standardized_df['CNTR_CODE'].unique()\n",
    "# unique_dou = standardized_df['dou'].unique()\n",
    "# discrete_colors = plt.cm.get_cmap('tab10_r', len(unique_dou))\n",
    "\n",
    "# # Create a strip (subplot) for each country\n",
    "# for i, cntr_code in enumerate(unique_countries):\n",
    "#     ax = fig.add_subplot(1, len(unique_countries), i + 1)\n",
    "\n",
    "#     # Add points for the decile_class inside the strip\n",
    "#     country_df = standardized_df[standardized_df['CNTR_CODE'] == cntr_code]\n",
    "#     for _, row in country_df.iterrows():        \n",
    "#         ax.scatter(\n",
    "#             0.5,  # Fixed x-coordinate for all points (center of the strip)\n",
    "#             row['decile_class'],  # Y position corresponds to decile_class\n",
    "#             color=discrete_colors(unique_dou.tolist().index(row['dou'])),\n",
    "#             s=50,  # Marker size\n",
    "#             alpha=0.8,\n",
    "#             label=f\"DOU {row['dou']}\" if f\"DOU {row['dou']}\" not in ax.get_legend_handles_labels()[1] else \"\"  # Add legend only once per DOU\n",
    "#         )\n",
    "    \n",
    "#     # Customize the strip\n",
    "#     ax.set_xlim(0, 1)  # Limit x-axis to create a strip effect\n",
    "#     ax.set_ylim(0.5, 10.5)  # Match y-axis range to decile_class\n",
    "#     ax.set_xticks([])  # Remove x-axis ticks\n",
    "#     ax.spines['top'].set_visible(True)  # Add the top spine\n",
    "#     ax.spines['right'].set_visible(True)  # Add the right spine\n",
    "#     ax.spines['bottom'].set_visible(True)  # Add the bottom spine\n",
    "#     ax.spines['left'].set_visible(True)  # Add the left spine\n",
    "#     ax.set_yticks(range(1, 11))  # Add y-axis ticks for decile_class\n",
    "#     if i == 0:\n",
    "#         ax.set_ylabel('Walkability Decile', fontsize=10)\n",
    "#     else:\n",
    "#         ax.set_yticklabels([])  # Remove y-axis labels for all but the first strip\n",
    "#     ax.set_xlabel(cntr_code, fontsize=10, labelpad=10)  # Add country name at the bottom\n",
    "\n",
    "# # Adjust the layout for better spacing\n",
    "# plt.subplots_adjust(wspace=0.4)\n",
    "\n",
    "# # Add a global legend\n",
    "# handles, labels = ax.get_legend_handles_labels()\n",
    "# fig.legend(\n",
    "#     handles=handles,\n",
    "#     labels=labels,\n",
    "#     title=\"Degree of Urbanization\",\n",
    "#     bbox_to_anchor=(0.5, -0.1),\n",
    "#     loc='lower center',\n",
    "#     ncol=6\n",
    "# )\n",
    "\n",
    "# # Save and display the plot\n",
    "# plt.savefig(\"strip_plot_countries_refined.svg\", format=\"svg\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0215238-bb57-4b12-9952-c353b321585f",
   "metadata": {},
   "source": [
    "#### plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f7e9fd-f8ff-4fa4-b926-0f2831dbe7cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data for latex tikz plot\n",
    "\n",
    "# list of EU-27 countries based on provided names\n",
    "eu_27_countries = [\n",
    "    \"AT\", \"BE\", \"BG\", \"HR\", \"CY\", \"CZ\", \"DK\", \"EE\", \"FI\", \"FR\", \"DE\", \"EL\", \n",
    "    \"HU\", \"IE\", \"IT\", \"LV\", \"LT\", \"LU\", \"MT\", \"NL\", \"PL\", \"PT\", \"RO\", \"SK\", \n",
    "    \"SI\", \"ES\", \"SE\", \"EU\"\n",
    "]\n",
    "\n",
    "# filter the dataset to keep only EU-27 countries\n",
    "df_eu27 = standardized_df[standardized_df['CNTR_CODE'].isin(eu_27_countries)]\n",
    "\n",
    "# group by country and walk_decile, then collect unique DOU values\n",
    "grouped_data = df_eu27.groupby(['CNTR_CODE', 'walk_decile'])['dou'].apply(list).reset_index()\n",
    "grouped_data.dropna(inplace=True)\n",
    "\n",
    "# create an ordered list of countries with 'EU' first\n",
    "ordered_countries = sorted(grouped_data['CNTR_CODE'].unique(), key=lambda x: (x != 'EU', x))\n",
    "\n",
    "# convert CNTR_CODE to a categorical with our custom ordering and re-sort\n",
    "grouped_data['CNTR_CODE'] = pd.Categorical(grouped_data['CNTR_CODE'], categories=ordered_countries, ordered=True)\n",
    "grouped_data.sort_values('CNTR_CODE', inplace=True)\n",
    "\n",
    "# now build the mapping for y-axis positions using the same ordering:\n",
    "country_index_map = {country: i+1 for i, country in enumerate(ordered_countries)}\n",
    "\n",
    "# check ordering by printing the unique values\n",
    "print(\", \".join(map(str, grouped_data.CNTR_CODE.unique())))\n",
    "\n",
    "# mapping for Degree of Urbanization to LaTeX color names\n",
    "dou_color_map = {\n",
    "    11: \"dou11\",\n",
    "    12: \"dou12\",\n",
    "    13: \"dou13\",\n",
    "    21: \"dou21\",\n",
    "    22: \"dou22\",\n",
    "    23: \"dou23\",\n",
    "    30: \"dou30\",\n",
    "}\n",
    "\n",
    "# generate LaTeX commands\n",
    "latex_commands = []\n",
    "for _, row in grouped_data.iterrows():\n",
    "    country = row['CNTR_CODE']\n",
    "    decile = row['walk_decile']\n",
    "    dou_list = row['dou']\n",
    "    \n",
    "    # convert DOU values to LaTeX color names\n",
    "    dou_colors = [dou_color_map[dou] for dou in dou_list if dou in dou_color_map]\n",
    "\n",
    "    # get country row index from our mapping\n",
    "    y_pos = country_index_map[country]\n",
    "\n",
    "    # generate LaTeX command if there are any valid colors\n",
    "    if dou_colors:\n",
    "        color_str = \",\".join(dou_colors)\n",
    "        latex_commands.append(f\"\\\\drawGroupPoints{{{decile}}}{{{y_pos}}}{{{len(dou_colors)}}}{{{color_str}}}\")\n",
    "\n",
    "# display the generated LaTeX code\n",
    "latex_code = \"\\n\".join(latex_commands)\n",
    "print(latex_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767343d-e511-49ff-8e20-e4ad036dd61e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### city analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d831112-ee5b-4192-b010-40def3de414a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lau_gdf = gpd.read_file('data/LAU_RG_01M_2023_3035.geojson')\n",
    "\n",
    "# clip by osm eu bbox\n",
    "osm_eu_gdf = gpd.read_file('data/osm_europe/osm_eu_bbox.shp').to_crs('epsg:3035')\n",
    "eu_gdf = lau_gdf[lau_gdf.intersects(osm_eu_gdf.geometry.values[0])]\n",
    "\n",
    "eu_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c69ac-d3f4-4ce2-8fc0-040cbb38bc63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_20_units = eu_gdf.sort_values(by='POP_2023', ascending=False).head(20)\n",
    "top_20_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37095fe9-624f-49a0-bb7d-a38ea3873f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_20_units.to_parquet('top_20_cities.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95676ad-15dd-4f0a-b9ba-d6afaf729903",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_gdfs = []\n",
    "\n",
    "for i, row in top_20_units.iterrows():\n",
    "    grid_gdf = gpd.read_parquet(f'data/walkability/grids_lau/grids_{row.GISCO_ID}.parquet')\n",
    "    grid_gdf['GISCO_ID'] = row.GISCO_ID\n",
    "    grid_gdf['LAU_Name'] = row.LAU_NAME\n",
    "    grid_gdfs.append(grid_gdf)\n",
    "\n",
    "t_20_grids_gdf = gpd.GeoDataFrame(pd.concat(grid_gdfs, ignore_index=True))\n",
    "t_20_grids_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358f039-7a7f-4248-a66b-cf4478ade012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zscore and build index\n",
    "columns_to_zscore = ['street_walk_length', 'iso_area',\n",
    "                     'num_street_intersections', 'pub_trans_count',\n",
    "                     'ndvi', 'slope', 'ent_5']\n",
    "\n",
    "t_20_grids_gdf[columns_to_zscore] = t_20_grids_gdf[columns_to_zscore].apply(zscore)\n",
    "t_20_grids_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24db7ad9-03de-42ec-a9fb-401b85b271ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_20_grids_gdf['walkability'] = (t_20_grids_gdf['street_walk_length'] +\n",
    "                     t_20_grids_gdf['iso_area'] +\n",
    "                     t_20_grids_gdf['num_street_intersections'] +\n",
    "                     t_20_grids_gdf['ndvi'] +\n",
    "                     t_20_grids_gdf['ent_5'] +\n",
    "                     (-t_20_grids_gdf['slope']) +\n",
    "                     t_20_grids_gdf['pub_trans_count'])\n",
    "t_20_grids_gdf['walk_index'] = ((t_20_grids_gdf['walkability'] - t_20_grids_gdf['walkability'].min()) / (t_20_grids_gdf['walkability'].max() - t_20_grids_gdf['walkability'].min()))*100\n",
    "t_20_grids_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff21c8-c9dc-4a10-a83e-b82ec005efff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decile_labels = [i+1 for i in range(10)]\n",
    "t_20_grids_gdf['walk_decile'] = pd.qcut(t_20_grids_gdf['walkability'], 10, labels=decile_labels)\n",
    "t_20_grids_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8acf9-6cc8-40e2-85aa-030d28764c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_20_grids_gdf.to_parquet('test_t_20_units.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587173bd-4b01-40cb-af92-c33bb5c7d280",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_20_grids_gdf = gpd.read_parquet('test_t_20_units.parquet')\n",
    "print(t_20_grids_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ffad51-9dae-4a10-bc80-b1ddade9aff8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped = t_20_grids_gdf.groupby('GISCO_ID')\n",
    "\n",
    "# placeholder for results\n",
    "results = []\n",
    "\n",
    "# loop through each group\n",
    "for group_id, group_gdf in grouped:\n",
    "    try:\n",
    "        # ensure geometries are valid\n",
    "        group_gdf = group_gdf[group_gdf.geometry.notnull()]\n",
    "        group_gdf = group_gdf[group_gdf.is_valid]\n",
    "\n",
    "        # skip empty or too-small groups\n",
    "        if group_gdf.shape[0] < 2:\n",
    "            print(f\"Skipping group {group_id}: too few observations\")\n",
    "            continue\n",
    "\n",
    "        # create spatial weights matrix (Queen contiguity)\n",
    "        w = Queen.from_dataframe(group_gdf)\n",
    "\n",
    "        # standardize the variable of interest\n",
    "        y = group_gdf['walk_index'].values \n",
    "        # calculate Moran's I\n",
    "        mi = Moran(y, w)\n",
    "\n",
    "        results.append({\n",
    "            'gisco_id': group_id,\n",
    "            'lau_name': group_gdf.LAU_Name.unique()[0],\n",
    "            'moran_I': mi.I,\n",
    "            'expected_I': mi.EI,\n",
    "            'p_value': mi.p_norm,\n",
    "            'z_score': mi.z_norm\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing group {group_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51336b-e0e0-4dfb-9e70-1a104c69ea4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451efc2e-59c1-4da0-89d6-b268506dec61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### city analysis population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902cfedb-017a-47ac-bf6e-f710d63dd115",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_20_gdf = gpd.read_parquet('top_20_cities.parquet')  # created from 'city analysis'\n",
    "print(top_20_gdf.LAU_NAME.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d4e61-044b-4128-8a0d-6009b7267d34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_gdfs = []\n",
    "\n",
    "for i, row in top_20_gdf.iterrows():\n",
    "    grid_gdf = gpd.read_parquet(f'data/walkability/grids_lau/grids_{row.GISCO_ID}.parquet')\n",
    "    grid_gdf['GISCO_ID'] = row.GISCO_ID\n",
    "    grid_gdf['LAU_NAME'] = row.LAU_NAME\n",
    "    grid_gdfs.append(grid_gdf)\n",
    "\n",
    "t_20_grids_gdf = gpd.GeoDataFrame(pd.concat(grid_gdfs, ignore_index=True))\n",
    "t_20_grids_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da575e6-b984-42a1-94ba-e8cc7b69bc21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# zscore and build index\n",
    "columns_to_zscore = ['street_walk_length', 'iso_area',\n",
    "                     'num_street_intersections', 'pub_trans_count',\n",
    "                     'ndvi', 'slope', 'ent_5']\n",
    "\n",
    "t_20_grids_gdf[columns_to_zscore] = t_20_grids_gdf[columns_to_zscore].apply(zscore)\n",
    "\n",
    "t_20_grids_gdf['walkability'] = (t_20_grids_gdf['street_walk_length'] +\n",
    "                     t_20_grids_gdf['iso_area'] +\n",
    "                     t_20_grids_gdf['num_street_intersections'] +\n",
    "                     t_20_grids_gdf['ndvi'] +\n",
    "                     t_20_grids_gdf['ent_5'] +\n",
    "                     (-t_20_grids_gdf['slope']) +\n",
    "                     t_20_grids_gdf['pub_trans_count'])\n",
    "t_20_grids_gdf['walk_index'] = ((t_20_grids_gdf['walkability'] - t_20_grids_gdf['walkability'].min()) / (t_20_grids_gdf['walkability'].max() - t_20_grids_gdf['walkability'].min()))*100\n",
    "\n",
    "decile_labels = [i+1 for i in range(10)]\n",
    "t_20_grids_gdf['walk_decile'] = pd.qcut(t_20_grids_gdf['walkability'], 10, labels=decile_labels)\n",
    "t_20_grids_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7ed020-94d7-4d2b-9e0c-0471c3a33ddb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bin_edges = np.arange(0, 101, 2)  # bins: 0-2, 2-4, ..., 98-100\n",
    "bin_labels = (bin_edges[:-1] + bin_edges[1:]) / 2  # midpoints for interpolation\n",
    "\n",
    "# group data and calculate population percentages\n",
    "g_t_20_grids_gdf = t_20_grids_gdf.groupby('LAU_NAME')\n",
    "results_dict = {}\n",
    "\n",
    "for lau_name, gdf in g_t_20_grids_gdf:\n",
    "    # assign each walk_index to a bin\n",
    "    gdf['walk_index_bin'] = np.digitize(gdf['walk_index'], bins=bin_edges, right=True) - 1\n",
    "    gdf['walk_index_bin'] = gdf['walk_index_bin'].apply(lambda x: bin_labels[x] if 0 <= x < len(bin_labels) else np.nan)\n",
    "\n",
    "    # group by binned walk_index and compute population percentages\n",
    "    pop_by_bin = gdf.groupby('walk_index_bin')['population_full'].sum()\n",
    "    total_population = pop_by_bin.sum()\n",
    "    pop_percentage = (pop_by_bin / total_population) * 100\n",
    "\n",
    "    results_dict[lau_name] = pop_percentage.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23947a52-da65-497e-867d-a92dbb0c9192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92b5e85-2be3-49ee-a58f-133ef7ab7f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define a function to translate the dictionary keys\n",
    "def translate_dict_keys(data_dict):\n",
    "    translation_map = {\n",
    "        \"Berlin, Stadt\": \"Berlin\",\n",
    "        \"Frankfurt am Main, Stadt\": \"Frankfurt\",\n",
    "        \"Grad Zagreb\": \"Zagreb\",\n",
    "        \"Hamburg, Freie und Hansestadt\": \"Hamburg\",\n",
    "        \"Krakw\": \"Krakow\",\n",
    "        \"Kln, Stadt\": \"Cologne\",\n",
    "        \"Municipiul Bucureti\": \"Bucharest\",\n",
    "        \"Mnchen, Landeshauptstadt\": \"Munich\",\n",
    "        \"Praha\": \"Prague\",\n",
    "        \"Valncia\": \"Valencia\",\n",
    "        \"Warszawa\": \"Warsaw\",\n",
    "        \"\": \"Sofia\"\n",
    "    }\n",
    "    \n",
    "    # rranslate dictionary keys\n",
    "    translated_dict = {translation_map.get(city, city): values for city, values in data_dict.items()}\n",
    "    return translated_dict\n",
    "\n",
    "translated_results_dict = translate_dict_keys(results_dict)\n",
    "translated_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb1cdc-71db-408c-bf6d-f97b4b674b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for poster only 10 cities\n",
    "# you can simply change this to see plot for all 20 cities\n",
    "\n",
    "poster_cities = ['Amsterdam', 'Barcelona', 'Berlin', 'Marseille', 'Munich', 'Oslo', 'Paris', 'Prague', 'Valencia', 'Warsaw']\n",
    "poster_dict = {k: translated_results_dict[k] for k in poster_cities if k in translated_results_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec5494-8633-4137-bb96-ab7178c1b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract GISCO IDs and assign colors\n",
    "gisco_ids = list(poster_dict.keys())\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(gisco_ids)))  # Unique color per GISCO ID\n",
    "# colors = plt.cm.tab20(np.linspace(0, 1, len(gisco_ids)))  # Unique color per GISCO ID -> 20 cities\n",
    "\n",
    "plt.figure(figsize=(6, 10))\n",
    "\n",
    "# define smoother x-values (continuous walk_index from 0 to 100)\n",
    "smooth_x = np.linspace(0, 100, 500)  # 500 points for smoothness\n",
    "\n",
    "# loop through each GISCO ID and plot its smooth curve\n",
    "for i, gisco_id in enumerate(gisco_ids):\n",
    "    walk_index_bins = np.array(list(poster_dict[gisco_id].keys()))\n",
    "    pop_counts = np.array(list(poster_dict[gisco_id].values()))\n",
    "\n",
    "    # sort by walk_index (x-axis)\n",
    "    sorted_indices = np.argsort(walk_index_bins)\n",
    "    sorted_walk_index = walk_index_bins[sorted_indices]\n",
    "    sorted_pop = pop_counts[sorted_indices]\n",
    "\n",
    "    # compute cumulative population percentage (y-axis)\n",
    "    cumulative_population = np.cumsum(sorted_pop)\n",
    "    cumulative_population = (cumulative_population / cumulative_population[-1]) * 100  # normalize to 0-100%\n",
    "\n",
    "    # interpolation function (cubic spline) with safe extrapolation limits\n",
    "    interp_func = interp1d(sorted_walk_index, cumulative_population, kind='cubic', bounds_error=False, fill_value=(0, 100))\n",
    "\n",
    "    # get smooth y-values and prevent values from going below 0 or above 100\n",
    "    smooth_y = np.clip(interp_func(smooth_x), 0, 100)\n",
    "\n",
    "    # plot smooth curve and filled area\n",
    "    plt.fill_between(smooth_x, smooth_y, alpha=0, color=colors[i])\n",
    "    plt.plot(smooth_x, smooth_y, label=gisco_id, color=colors[i], linewidth=2)\n",
    "\n",
    "# customize plot\n",
    "plt.yticks(np.arange(0, 110, 10))\n",
    "plt.xticks(np.arange(0, 110, 10))\n",
    "\n",
    "plt.xlabel(\"Walkability Index\")\n",
    "plt.ylabel(\"Cumulative Population %\")\n",
    "plt.ylim(0, 100)  # Explicitly enforce Y-axis limits\n",
    "plt.xlim(30, 100)\n",
    "plt.legend(loc=\"upper left\")\n",
    "# plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.savefig(\"plots_paper/curve_poster.svg\", format=\"svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2d74f-b112-4691-a0ee-4b54669404ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data and calculate population percentages\n",
    "g_t_20_grids_gdf = t_20_grids_gdf.groupby('LAU_NAME')\n",
    "n_results_dict = {}\n",
    "\n",
    "for lau_name, gdf in g_t_20_grids_gdf:\n",
    "    # group by binned walk_index and compute population percentages\n",
    "    pop_decile = gdf.groupby('walk_decile')['population_full'].sum()\n",
    "    total_population = pop_decile.sum()\n",
    "    pop_percentage = (pop_decile / total_population) * 100\n",
    "\n",
    "    # store results in dictionary with GISCO ID\n",
    "    n_results_dict[lau_name] = pop_percentage.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0700ea51-912b-4216-81a8-f1d925667542",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "translated_n_results_dict = translate_dict_keys(n_results_dict)\n",
    "translated_n_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b8be7-ba1b-496b-897d-9a8e72381678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the total percent of people below the 6th decile\n",
    "city_names = []\n",
    "total_below_6th_decile = []\n",
    "\n",
    "for city, deciles in translated_n_results_dict.items():\n",
    "    total = sum(deciles[d] for d in range(1, 6))  # sum values for 1st to 5th decile\n",
    "    city_names.append(city)\n",
    "    total_below_6th_decile.append(total)\n",
    "\n",
    "# sort the data for better visualization\n",
    "sorted_indices = sorted(range(len(total_below_6th_decile)), key=lambda i: total_below_6th_decile[i])\n",
    "sorted_city_names = [city_names[i] for i in sorted_indices]\n",
    "sorted_totals = [total_below_6th_decile[i] for i in sorted_indices]\n",
    "\n",
    "# create the horizontal bar chart with styling similar to the provided image\n",
    "fig, ax = plt.subplots(figsize=(6, 10)) \n",
    "\n",
    "bars = ax.barh(sorted_city_names, sorted_totals, color='orange', edgecolor='black', height=0.5)\n",
    "\n",
    "# add percentages as text labels\n",
    "max_value = max(sorted_totals)\n",
    "for bar, value in zip(bars, sorted_totals):\n",
    "    ax.text(bar.get_width() + max_value * 0.02, bar.get_y() + bar.get_height()/2, f\"{value:.1f}%\", va='center')\n",
    "\n",
    "# styling the plot\n",
    "ax.set_xlim(0, max_value * 1.13) \n",
    "ax.set_xlabel(r\"% Pop <6th decile\")\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "plt.savefig(\"plots_paper/barh.svg\", format=\"svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd5db7-606f-4c24-b21c-1d2a8ba5e131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
