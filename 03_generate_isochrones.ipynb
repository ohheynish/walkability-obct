{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eb0e112-6a11-435b-8cf3-a16645b4a605",
   "metadata": {},
   "source": [
    "This notebook contains code to generate isochrones for 100m x 100m grids using Valhalla's open source routing engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44ce22-4b5a-45dd-9512-f5a4b232c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon, box, shape\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "import logging\n",
    "import shutil\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import glob\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066dd85e-f7be-4510-a506-429b95acdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for grid files\n",
    "# you might have grids generated here from the 'generate_grids.ipynb'\n",
    "grid_path = 'data/*.parquet'\n",
    "\n",
    "# create a list of all parquet grid files from the specified directory\n",
    "grids_list = [parquet for parquet in glob.glob(grid_path)]\n",
    "print(grids_list)\n",
    "\n",
    "# # configure logging (recommended if you monitor processing over a lot of files)\n",
    "# log_path = 'logs/isochrones.log'\n",
    "\n",
    "# # ensure log directory exists\n",
    "# log_dir = os.path.dirname(log_path)\n",
    "# if not os.path.exists(log_dir):\n",
    "#     os.makedirs(log_dir)\n",
    "    \n",
    "# logging.basicConfig(filename=log_path, level=logging.INFO,\n",
    "#                     format='%(asctime)s:%(levelname)s:%(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37db179a-5b4e-4767-a536-611ef1067efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # file to store failed indices\n",
    "# failed_indices_file = './failed_isochrones_indices.pkl'\n",
    "\n",
    "# # load existing failed indices if the file exists\n",
    "# try:\n",
    "#     with open(failed_indices_file, 'rb') as f:\n",
    "#         failed_indices_dict = pickle.load(f)\n",
    "# except FileNotFoundError:\n",
    "#     failed_indices_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af408f77-3b50-4a0e-b6cf-063ad7d61649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_tar_file(file_path, timeout=3600, check_interval=5):\n",
    "    start_time = time.time()\n",
    "    while True:\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"File found: {file_path}\")\n",
    "            return True\n",
    "        elif (time.time() - start_time) > timeout:\n",
    "            print(f\"Timeout: File {file_path} not found within {timeout} seconds.\")\n",
    "            return False\n",
    "        else:\n",
    "            # print(f\"File {file_path} not found, checking again in {check_interval} seconds...\")\n",
    "            time.sleep(check_interval)\n",
    "\n",
    "\n",
    "def get_isochrone(lat, lon, duration=15):\n",
    "    \"\"\"\n",
    "    Make an HTTP POST request to retrieve isochrone data for given coordinates.\n",
    "    \"\"\"\n",
    "    url = 'http://localhost:8002/isochrone'\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"locations\": [{\"lat\": lat, \"lon\": lon}],\n",
    "        \"costing\": \"pedestrian\",\n",
    "        \"contours\": [{\"time\": duration}],\n",
    "        \"polygons\": True\n",
    "    }\n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        # Convert the GeoJSON response to a Shapely geometry\n",
    "        geojson = response.json()\n",
    "        geometry = shape(geojson['features'][0]['geometry'])\n",
    "        return geometry\n",
    "    else:\n",
    "        return Polygon()\n",
    "\n",
    "def send_isochrone_request(row):\n",
    "    if row.street_walk_length != 0:\n",
    "        return get_isochrone(row.centroid.y, row.centroid.x)\n",
    "    else:\n",
    "        return Polygon()\n",
    "\n",
    "def save_failed_indices(grid_id, failed_indices):\n",
    "    \"\"\"\n",
    "    Save failed indices for a grid to the pickle file.\n",
    "\n",
    "    Args:\n",
    "    - grid_id: The ID of the grid being processed.\n",
    "    - failed_indices: List of indices that failed.\n",
    "    \"\"\"\n",
    "    global failed_indices_dict\n",
    "    if grid_id not in failed_indices_dict:\n",
    "        failed_indices_dict[grid_id] = []\n",
    "    failed_indices_dict[grid_id].extend(failed_indices)\n",
    "\n",
    "    # remove duplicates\n",
    "    failed_indices_dict[grid_id] = list(set(failed_indices_dict[grid_id]))\n",
    "\n",
    "    # save to file\n",
    "    with open(failed_indices_file, 'wb') as f:\n",
    "        pickle.dump(failed_indices_dict, f)\n",
    "\n",
    "def process_isochrones(gdf):\n",
    "    \"\"\"\n",
    "    Process isochrones for a given GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "    - gdf: GeoDataFrame containing grid data.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary of isochrones indexed by grid index.\n",
    "    \"\"\"\n",
    "    isochrones = {}\n",
    "    failed_indices = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        future_to_index = {executor.submit(send_isochrone_request, row): index for index, row in gdf.iterrows()}\n",
    "        for future in as_completed(future_to_index):\n",
    "            index = future_to_index[future]\n",
    "            try:\n",
    "                isochrones[index] = future.result()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Request failed for index {index}: {e}\")\n",
    "                # record the failed index and return an empty Polygon for it\n",
    "                failed_indices.append(index)\n",
    "                isochrones[index] = Polygon()\n",
    "\n",
    "    # log failed indices for the grid\n",
    "    if failed_indices:\n",
    "        grid_id = str(gdf['grid_100000_id'].unique()[0])\n",
    "        save_failed_indices(grid_id, failed_indices)\n",
    "        logging.info(f\"Failed indices for grid {grid_id} saved to {failed_indices_file}\")\n",
    "\n",
    "    return isochrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af1b61-8276-4737-b134-0fb1ef8ef4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "europe_osm_pbf_path = '/path/to/europe-latest.osm.pbf'\n",
    "europe_bbox_shp_path = '/path/to/osm_eu_bbox.shp'\n",
    "\n",
    "def process_grid(grid_path):\n",
    "    \"\"\"\n",
    "    Processes each grid to generate isochrones.\n",
    "    \n",
    "    Args:\n",
    "    - grid_path (str): Path to the grid file.\n",
    "    \"\"\"\n",
    "    grid_gdf = gpd.read_parquet(grid)\n",
    "    grid_id = str(grid_gdf['grid_100000_id'].unique()[0])\n",
    "    \n",
    "    if 'street_walk_length' not in grid_gdf.columns:\n",
    "        logging.info(f'skipping grid {grid_id} as no street walk length')\n",
    "        print(f'skipping grid {grid_id} as no street walk length')\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # define output directory and file\n",
    "        output_dir = 'data/isochrones'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_file = f'isochrones_{grid_id}.parquet'\n",
    "\n",
    "        # here also check from failed dict indices and filter the gdf accordingly?\n",
    "        if os.path.exists(os.path.join(output_dir, output_file)) and grid_id not in failed_indices_dict.keys():\n",
    "            logging.info(f'skipping grid {grid_id} as isochrone already calculated')\n",
    "            print(f'skipping grid {grid_id} as isochrone already calculated')\n",
    "            return\n",
    "            \n",
    "        if grid_id in failed_indices_dict.keys():\n",
    "            grid_gdf = grid_gdf.loc[failed_indices_dict[grid_id]]\n",
    "            output_file = f'isochrones_{grid_id}_failed_indices.parquet'\n",
    "    \n",
    "        # make a buffer to cut europe.osm.pbf\n",
    "        bounds_grid_gdf = grid_gdf.total_bounds\n",
    "        bounds_box = box(bounds_grid_gdf[0], bounds_grid_gdf[1], bounds_grid_gdf[2], bounds_grid_gdf[3])\n",
    "        buff_bounds_box = bounds_box.buffer(3000)\n",
    "    \n",
    "        buff_gdf = gpd.GeoDataFrame(geometry=[buff_bounds_box], crs=grid_gdf.crs)\n",
    "        buff_gdf_4326 = buff_gdf.to_crs('epsg:4326')\n",
    "        buff_bounds_4326 = buff_gdf_4326.total_bounds\n",
    "        \n",
    "        # check if grid_gdf is within the osm europe polygon\n",
    "        osm_eu_bbox_gdf = gpd.read_file(europe_bbox_shp_path)\n",
    "        if osm_eu_bbox_gdf.geometry.values[0].contains(buff_gdf_4326.geometry.values[0]):\n",
    "            # osmium extract pbf\n",
    "            os.makedirs(f'data/custom_files_{grid_id}')\n",
    "            logging.info(f'started processing grid {grid_id}...')\n",
    "            print(f'started processing grid {grid_id}...')\n",
    "            \n",
    "            clipped_pbf_path = f'data/custom_files_{grid_id}/{grid_id}.pbf'\n",
    "            bbox = f\"{buff_bounds_4326[0]},{buff_bounds_4326[1]},{buff_bounds_4326[2]},{buff_bounds_4326[3]}\"\n",
    "            osmium_result = subprocess.run([\n",
    "                                            'osmium', 'extract',\n",
    "                                            f'-b{bbox}',\n",
    "                                            europe_osm_pbf_path,\n",
    "                                            '-o', clipped_pbf_path\n",
    "                                        ], check=True)\n",
    "        \n",
    "            if osmium_result.returncode == 0:\n",
    "                logging.info(f'successfully clipped osm.pbf for grid {grid_id}; initiating docker...')\n",
    "                print(f'successfully clipped osm.pbf for grid {grid_id}; initiating docker...')\n",
    "        \n",
    "                # run docker\n",
    "                custom_files_path = f'/Volumes/ssd1/custom_files_{grid_id}:/custom_files'\n",
    "                docker_result = subprocess.run([\n",
    "                                                'docker', 'run', '-dt', '--name', f'valhalla_{grid_id}',\n",
    "                                                '-p', '8002:8002',\n",
    "                                                '-v', custom_files_path,\n",
    "                                                'ghcr.io/gis-ops/docker-valhalla/valhalla:latest'\n",
    "                                            ], check=True)\n",
    "        \n",
    "                if docker_result.returncode == 0:\n",
    "                    # wait for .tar file\n",
    "                    tar_file = f'data/custom_files_{grid_id}/valhalla_tiles.tar'\n",
    "                    if wait_for_tar_file(tar_file):\n",
    "                        logging.info(f'successfully created valhalla tiles for grid {grid_id}; sending isochrone reqs...')\n",
    "                        print(f'successfully created valhalla tiles for grid {grid_id}; sending isochrone reqs...')\n",
    "        \n",
    "                        grid_gdf['centroid'] = grid_gdf.centroid\n",
    "                        grid_gdf = grid_gdf.set_geometry('centroid')\n",
    "                        grid_gdf_4326 = grid_gdf.to_crs('epsg:4326')\n",
    "                        \n",
    "                        start_time = time.time()  # start timing\n",
    "                        \n",
    "                        # start processing isochrones\n",
    "                        isochrones = process_isochrones(gdf=grid_gdf_4326)\n",
    "                        \n",
    "                        end_time = time.time()  # end timing\n",
    "                        elapsed_time = end_time - start_time\n",
    "                        logging.info(f'isochrone calc for grid {grid_id} took {elapsed_time} seconds')\n",
    "                        \n",
    "                        # clean up\n",
    "                        subprocess.run(['docker', 'stop', f'valhalla_{grid_id}'], check=True)\n",
    "                        subprocess.run(['docker', 'rm', f'valhalla_{grid_id}'], check=True)\n",
    "                        subprocess.run(['rm', '-rf', f'data/custom_files_{grid_id}'], check=True)\n",
    "                        \n",
    "                        # this assertion wont work for the grids in the failed_indices_dict!\n",
    "                        # assert len(isochrones) == 1_000_000, 'length of isochrones is not equal to the length of grid_gdf'\n",
    "                \n",
    "                        # save isochornes\n",
    "                        iso_gdf = gpd.GeoDataFrame({'geometry': [isochrones[k] for k in sorted(isochrones.keys())]},\n",
    "                                                   index=sorted(isochrones.keys()))\n",
    "        \n",
    "                        iso_gdf.set_crs('epsg:4326', inplace=True)\n",
    "                        iso_gdf.to_crs('epsg:3035', inplace=True)\n",
    "                        iso_gdf.reset_index(inplace=True)\n",
    "                        iso_gdf.to_parquet(os.path.join(output_dir, output_file))\n",
    "        \n",
    "                        logging.info(f'successfully generated isochrones for grid {grid_id}; cleaning up...')\n",
    "                        print(f'successfully generated isochrones for grid {grid_id}; cleaning up...')\n",
    "\n",
    "                    else:\n",
    "                        logging.error(f'failed to create valhalla tiles for grid {grid_id}')\n",
    "                        print(f'failed to create valhalla tiles for grid {grid_id}')\n",
    "            else:\n",
    "                logging.error(f'osmium extraction error for grid {grid_id}')\n",
    "                print(f'osmium extraction error for grid {grid_id}')\n",
    "                return\n",
    "        else:\n",
    "            logging.info(f'grid {grid_id} is outside osm eu bbox; skipping...')\n",
    "            print(f'grid {grid_id} is outside osm eu bbox; skipping...')\n",
    "            return\n",
    "    except Exception as e:\n",
    "        logging.error(f'Error processing grid {grid_id}: {e}')\n",
    "        print(f'Error processing grid {grid_id}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fade63a-84b7-4ee9-883b-657e05721a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential\n",
    "for elem in grids_list:\n",
    "    process_grid(elem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
