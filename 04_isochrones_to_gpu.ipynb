{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61258c0-418b-4b7c-86c1-e7ada07a2377",
   "metadata": {},
   "source": [
    "This notebook contains code on how to convert the isochrone polygons (and other component data) to arrays, which can be then processed parallely on GPU for stuff like weighted distance decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae10bc-7c0f-4636-ac01-1cfb658710a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from geocube.api.core import make_geocube\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import os\n",
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a4ffe6-61d9-4187-989c-2e84ca5b3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for grid files\n",
    "# you might have grids generated here from the 'generate_grids.ipynb'\n",
    "grid_path = 'data/*.parquet'\n",
    "\n",
    "# create a list of all parquet grid files from the specified directory\n",
    "grids_list = [parquet for parquet in glob.glob(grid_path)]\n",
    "print(grids_list)\n",
    "\n",
    "# # configure logging (recommended if you monitor processing over a lot of files)\n",
    "# log_path = 'logs/isochrones_to_gpu.log'\n",
    "\n",
    "# # ensure log directory exists\n",
    "# log_dir = os.path.dirname(log_path)\n",
    "# if not os.path.exists(log_dir):\n",
    "#     os.makedirs(log_dir)\n",
    "    \n",
    "# logging.basicConfig(filename=log_path, level=logging.INFO,\n",
    "#                     format='%(asctime)s:%(levelname)s:%(message)s', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1d99b7-b341-482e-91a4-82f23749d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(grid_gdf, grid_100km_gdf, x_cols):\n",
    "    \"\"\"\n",
    "    Find neighbors of a grid cell by buffering its boundaries and identifying intersecting grids.\n",
    "    \n",
    "    Args:\n",
    "    - grid_gdf (GeoDataFrame): GeoDataFrame of the grid to find neighbors for.\n",
    "    - grid_100km_gdf (GeoDataFrame): 100kmx100km GeoDataFrame consisting all grid ids.\n",
    "    - x_cols (List): List of walkability components.\n",
    "    \n",
    "    Returns:\n",
    "    - List[GeoDataFrame]: List of GeoDataFrames of the neighboring grid cells.\n",
    "    \"\"\"\n",
    "    grid_id = grid_gdf['grid_100000_id'].unique()\n",
    "    buffer = grid_gdf.unary_union.buffer(3000, cap_style=3)\n",
    "    potential_neighbors = grid_100km_gdf[grid_100km_gdf.intersects(buffer)]\n",
    "    \n",
    "    nbr_gdfs = []\n",
    "    for neighbor_id in set(potential_neighbors.index) - set(grid_id):\n",
    "        temp_nbr_gdf = gpd.read_parquet(f'data/grids_100_{neighbor_id}.parquet')\n",
    "        temp_nbr_gdf = temp_nbr_gdf[temp_nbr_gdf.intersects(buffer)]\n",
    "\n",
    "        # check if any data col is missing\n",
    "        # if missing we replace it by 0 (this is only limited to neighbors so shouldn't be an issue?)\n",
    "        if not set(x_cols).issubset(set(temp_nbr_gdf.columns)):        \n",
    "            missing_cols = list(set(x_cols) - set(temp_nbr_gdf.columns))\n",
    "            nbr_id = temp_nbr_gdf['grid_100000_id'].unique()\n",
    "            print(f'{missing_cols} missing for {grid_id} nbr - {nbr_id}, replacing with 0...')\n",
    "            # logging.info(f'{missing_cols} missing for {grid_id} nbr - {nbr_id}, replacing with 0...')\n",
    "            for missing_col in missing_cols:\n",
    "                temp_nbr_gdf[missing_col] = 0.0\n",
    "        \n",
    "        nbr_gdfs.append(temp_nbr_gdf)\n",
    "    \n",
    "    return nbr_gdfs\n",
    "\n",
    "def rasterize_geodf(geodf, columns, resolution):\n",
    "    \"\"\"\n",
    "    Rasterizes specified columns of a GeoDataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - geodf (GeoDataFrame): GeoDataFrame to rasterize.\n",
    "    - columns (List[str]): List of column names to rasterize.\n",
    "    - resolution (tuple): The pixel resolution in the form of (width, height).\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple: A tuple containing the rasterized data array, transform, and CRS.\n",
    "    \"\"\"\n",
    "    cube = make_geocube(vector_data=geodf, measurements=columns, resolution=resolution, output_crs=\"EPSG:3035\")\n",
    "    bands = [cube[col].values for col in columns]  # test if 'cube[col].data' is faster?\n",
    "    return np.stack(bands), cube.rio.transform(), cube.rio.crs\n",
    "\n",
    "def process_grid(grid_path, grid_100km_gdf, x_cols):\n",
    "    \"\"\"\n",
    "    Process each grid to find neighbors, merge data, and create a raster image for GPU processing.\n",
    "    \n",
    "    Args:\n",
    "    - grid_path (str): Path to the grid file.\n",
    "    \"\"\"\n",
    "    grid_gdf = gpd.read_parquet(grid_path)\n",
    "    grid_num = grid_gdf['grid_100000_id'].unique()[0]\n",
    "    \n",
    "    output_dir = 'data/iso_for_gpu'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_indexing_arr_file_path = os.path.join(output_dir, f'indexing_arr_{grid_num}.npz')\n",
    "    output_img_file_path = os.path.join(output_dir, f'img_{grid_num}.tif')\n",
    "    \n",
    "    if os.path.exists(os.path.join(output_dir, output_img_file_path)):\n",
    "        print(f'Skipping grid {grid_num} as iso_gpu already calculated')\n",
    "        return\n",
    "    \n",
    "    # some component from main grid missing\n",
    "    if not set(x_cols).issubset(set(grid_gdf.columns)):\n",
    "        print(f'skipping grid {grid_num} as w_col missing: main_grid_id')\n",
    "        # logging.info(f'skipping grid {grid_num} as w_col missing: main_grid_id')\n",
    "        return\n",
    "    \n",
    "    nbr_gdfs = find_neighbors(grid_gdf=grid_gdf, grid_100km_gdf=grid_100km_gdf, x_cols=x_cols)\n",
    "    new_grid_gdf = gpd.GeoDataFrame(pd.concat([grid_gdf] + nbr_gdfs, ignore_index=True))\n",
    "    new_grid_gdf['retain_nbr_geom'] = new_grid_gdf.geometry\n",
    "    new_grid_gdf.drop(columns=['index'], inplace=True)\n",
    "    \n",
    "    iso_path = f'data/isochrones/isochrones_{grid_num}.parquet'\n",
    "    iso_gdf = gpd.read_parquet(iso_path)\n",
    "\n",
    "    # wherever iso has empty polygon fill with normal buffer polygon\n",
    "    # this leads to uncertainty? (as normal buffer would contain larger amounts of nbrs and\n",
    "    # thus more costlier weighted reduction in kernel? however this is best to avoid outliers)\n",
    "    empty_pols = iso_gdf[iso_gdf.geometry.is_empty].index\n",
    "    condition = (grid_gdf.loc[empty_pols]['population'] > 0) | (grid_gdf.loc[empty_pols]['street_walk_length'] > 0)\n",
    "    filtered_entries = grid_gdf.loc[empty_pols][condition]\n",
    "    buffered_pols = filtered_entries.geometry.centroid.buffer(1200) \n",
    "    iso_gdf.loc[filtered_entries.index, 'geometry'] = buffered_pols\n",
    "\n",
    "    iso_gdf.drop(columns=['index'], inplace=True)\n",
    "\n",
    "    joined_gdf = iso_gdf.sjoin(new_grid_gdf, how='left', predicate='intersects')\n",
    "    joined_gdf.reset_index(inplace=True)\n",
    "\n",
    "    joined_gdf_x = joined_gdf[['index', 'index_right']].copy()\n",
    "    joined_gdf_x.set_index('index', inplace=True)\n",
    "\n",
    "    # convert to an array of size (num neighborhoods, indices of local intersecting neighbors)\n",
    "    joined_gdf_x['group_num'] = joined_gdf_x.groupby(joined_gdf_x.index).cumcount()\n",
    "    res_df = joined_gdf_x.pivot(columns='group_num', values='index_right')\n",
    "    res_df.reset_index(inplace=True)\n",
    "    res_df.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "    # save indexing arr\n",
    "    indexing_arr = res_df.to_numpy()\n",
    "    print(indexing_arr.shape)\n",
    "    np.savez_compressed(output_indexing_arr_file_path, array=indexing_arr)\n",
    "\n",
    "    new_grid_gdf.reset_index(inplace=True)\n",
    "\n",
    "    # save image data to be processed (new_grid_gdf + index from grid_gdf)\n",
    "    f_grid_bands, f_grid_transform, f_grid_crs = rasterize_geodf(new_grid_gdf, x_cols, resolution=(-100, 100))\n",
    "    grid_bands, grid_transform, grid_crs = rasterize_geodf(grid_gdf, x_cols[-1], resolution=(-100, 100))\n",
    "\n",
    "    # calculate offset for placeing grid_bands into f_grid_bands\n",
    "    grid_xmin, grid_ymin, grid_xmax, grid_ymax = grid_gdf.total_bounds\n",
    "    f_grid_xmin, f_grid_ymin, f_grid_xmax, f_grid_ymax = new_grid_gdf.total_bounds\n",
    "    x_offset = int((grid_xmin - f_grid_xmin) / abs(f_grid_transform[0]))\n",
    "    y_offset = int((f_grid_ymax - grid_ymax) / abs(f_grid_transform[4]))\n",
    "\n",
    "    # extract the dimensions of the reasterized gdf\n",
    "    height, width = f_grid_bands.shape[1], f_grid_bands.shape[2]\n",
    "    channels = len(x_cols) + len([x_cols[-1]])\n",
    "\n",
    "    # intialize an empty array to hold the composite bands\n",
    "    composite_bands = np.full((channels, height, width), np.nan, dtype=np.float32)\n",
    "\n",
    "    # fill composite bands with nbr_bands data\n",
    "    composite_bands[:channels-1, :, :] = f_grid_bands\n",
    "\n",
    "    # fill composite bands with grid_bands data\n",
    "    composite_bands[-1, y_offset:y_offset+grid_bands.shape[1], x_offset:x_offset + grid_bands.shape[2]] = grid_bands\n",
    "    print(composite_bands.shape)\n",
    "    \n",
    "    # create a profile for the raster file\n",
    "    profile = {\n",
    "        'driver': 'GTiff',\n",
    "        'count': composite_bands.shape[0],\n",
    "        'dtype': composite_bands.dtype,\n",
    "        'width': width,\n",
    "        'height': height,\n",
    "        'crs': f_grid_crs.to_string(),\n",
    "        'transform': f_grid_transform\n",
    "    }\n",
    "    \n",
    "    # write the raster to a single TIFF file with multiple bands\n",
    "    with rasterio.open(output_img_file_path, 'w', **profile) as dst:\n",
    "        for i in range(composite_bands.shape[0]):\n",
    "            dst.write(composite_bands[i], i + 1)\n",
    "    \n",
    "    print(f'processed {grid_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e649d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = ['green_area', 'street_walk_length', 'num_street_intersections', 'ndvi', 'ent_5',\n",
    "          'slope', 'population', 'pub_trans_count', 'index']\n",
    "\n",
    "# 100km grid_gdf\n",
    "grid_100km = gpd.read_file('data/grid_100km_surf.gpkg')\n",
    "grid_100km.index += 1\n",
    "grid_100km.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe61f41-4029-4918-b8f2-861720b32145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential\n",
    "for elem in grids_list:\n",
    "    process_grid(elem, grid_100km_gdf=grid_100km, x_cols=x_cols)\n",
    "\n",
    "# # parallel\n",
    "# num_processes = 5\n",
    "\n",
    "# with Pool(processes=num_processes) as pool:\n",
    "#     pool.map(process_grid, grids_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
